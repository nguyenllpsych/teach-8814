---
title: "PSY 8814 - Lab 10: Paired-samples t-test and ANOVA"
author: "Linh Nguyen"
date: "2023-11-10"
output: 
  pdf_document:
    toc: TRUE
urlcolor: blue
---

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      fig.height = 4)
```

```{r library, message = FALSE, warning = FALSE}
library(dplyr)      # for general wrangling and pipe
library(ggplot2)    # for plotting
library(gridExtra)  # for arranging ggplot
library(kableExtra) # for table formatting
```


# I. Paired-Sampled $t$-Test

Last week, we covered the independent-samples $t$-test, which is used to compare the means of two samples which come from two different populations that are independent from one another. However, sometimes, our two samples are linked in some interesting way. For example, they might be couples, twins, or even the same people (e.g., before and after treatment, in childhood versus adulthood, etc.). We conduct a hypothesis test using the difference scores for $\mathbf{x}$ and $\mathbf{y}$. Sometimes you will see the difference score constructed as $\mathbf{z} = \mathbf{x} - \mathbf{y}$.

For example, in the `sleep` dataframe in the `{datasets}` package, we have $N=10$ individuals who were each given two types of sleep medications. The outcome of interest is the "increase in hours of sleep" after being administered each medication type. This variable is named `extra` in the data set. The data come in long format, with each ID listed twice for two medication types.

```{r data-ttest}
# Load data (comes by default in R {datasets})
data("sleep")
head(sleep, n = 12)
```

Let the random variable $Z = X - Y$, where $X$ is the outcome for medication 1, and $Y$ is the outcome variable for medication 2. We will test the following hypotheses:

$$
H_0: \mu_z = 0 \quad \text{versus} \quad H_1: \mu_z \neq 0
$$

```{r data-ttest-vec}
# Subset data and create 2 vectors for 2 groups
med1 <- sleep$extra[sleep$group == 1]
med2 <- sleep$extra[sleep$group == 2]

# Construct difference scores
extra_diff <- med2 - med1
extra_diff
```

## 1. Assumption checks

We'll first explore the data and check our assumptions.

```{r desc-ttest}
# Boxplot of DV by pill type
plot_group <- ggplot(data = sleep, aes(x = group, y = extra)) +
  geom_boxplot(fill = "blue", alpha = 0.2) +
  labs(
    title = "Boxplot of Sleeping Data",
    x = "Medication type",
    y = "Increase in hours of sleep"
  ) +
  theme_classic()

# Boxplot of difference scores
plot_diff <- ggplot(, aes(x = extra_diff)) +
  geom_boxplot(fill = "blue", alpha = 0.2) +
  coord_flip() +
  labs(
    title = "Boxplot of Sleeping Data",
    x = "Difference Score",
    y = "Increase in hours of sleep"
  ) +
  theme_classic()

# organize 2 plots next to each other
grid.arrange(plot_group, plot_diff, ncol = 2)

# Q-Q plots
qqnorm(med1, main = "Med 1"); qqline(med1)
qqnorm(med2, main = "Med 2"); qqline(med2)
qqnorm(extra_diff, main = "Difference"); qqline(extra_diff)

# Skew and kurtosis 
cbind(
  
  e1071::skewness(med1, type = 2),
  e1071::kurtosis(med1, type = 1),
  e1071::skewness(med2, type = 2),
  e1071::kurtosis(med2, type = 1),
  e1071::skewness(extra_diff, type = 2),
  e1071::kurtosis(extra_diff, type = 1)
  
) %>%
  kable(format = "latex", booktabs = T,
        col.names = rep(c("Skew", "Kurtosis"), 3),
        caption = "Skew and kurtosis for raw data and difference scores") %>%
  kable_styling(latex_options = "hold_position") %>%
  add_header_above(c("Medication 1" = 2, "Medication 2" = 2, "Difference Scores" = 2))
```

## 2. Conduct the test with `t.test()`

With only 10 paired observations, it can be difficult to make claims about our assumptions. For example, we might see more evidence for normality just by increasing the sample size! Here, we can see slight differences in the distributions of the dependent variable by pill type. There appears to be an outlying difference score based on the boxplot. The difference scores also appear positively skewed with these data.

```{r ttest}
# Run t test
# Remember that our data is long, so can use the tilde sign
#   general syntax: outcome variable ~ grouping variable
t.test(extra ~ group, data = sleep, paired = TRUE)

# Alternative way with 2 separate vectors of data
#   remember, the data are paired, so the order matters!
t.test(med1, med2, paired = TRUE)
```

The *p*-value is less than $\alpha=0.05$, so we reject the null hypothesis that the mean increase in hours slept are equivalent between the two treatment conditions.

# II. One-Way ANOVA Test

The one-way ANOVA is an extension of the two-sample $t$-test to more than two groups. ANOVA is often used to examine treatment effects with more than two treatment conditions. For example, we might be interested in individuals' perceptions of four different brands of coffee. We can use a one-way ANOVA to compare the mean rating scores among the four brands. 

There are important assumptions underlying an ANOVA. First, we assume constant variance, such that $\sigma^2$ is the same for all groups. We also assume that the observations are normally distributed. In other words, the observations within each group are normally-distributed with some mean, $\mu_j$, and a constant variance term. We use ANOVA to make an inference about the vector of means. 

With ANOVA, we first conduct an omnibus test to see if there are any mean differences among the groups, with the $F$-test which uses the $F$-statistic. A significant $F$-test indicates that at least one of the means differs from the others, but we can't say which one! We can then do pairwise comparisons as a post-hoc analysis.

## 1. Sum of Squares Between

The **sum of squares between** (SSB) measures the sum of squared differences between each group mean (i.e., using the plot below, the peak of each of the three distributions) and the grand mean (i.e., the mean of all observations together). In the below plot, the dashed line represents the grand mean, and each curve represents a different group distribution.

```{r plot-ssb}
# Plot of three distributions with grand mean
x = seq(-4, 4, length.out = 1000)
sigma2 = 1
mu1 = 0; mu2 = -1; mu3 = 2

ggplot() +
  # add density lines for each distribution
  geom_line(aes(x = x, y = dnorm(x, mu1, sigma2)), 
            col = "blue", lwd = 1.2) +
  geom_line(aes(x = x, y = dnorm(x, mu2, sigma2)), 
            col = "orange", lwd = 1.2) +
  geom_line(aes(x = x, y = dnorm(x, mu3, sigma2)), 
            col = "maroon", lwd = 1.2) +
  # add overall grand mean
  geom_vline(xintercept = 1, lty = "dashed") +
  labs(
    title = "Plots of three distributions",
    subtitle = "dashed line = grand means across all groups",
    y = "density"
  ) +
  theme_classic()
```

## 2. Sum of Squares Within

The **sum of squares within** (SSW) quantifies the variation of each observation from its group mean. In the below plot, the dashed lines represent group means for three groups. So here, we compute the sum of squared differences between each individual observation and its corresponding group mean. The difference between the observation and the mean is often referred to as a *deviation* score. So for all observations following the orange (or farthest left) distribution, we compute the deviation score as the difference between the observation and the mean of this group's distribution (in this case, -1). Sum up all the squared deviation scores, and we get the SSW.


```{r plot-ssw}
# Plot of three distributions with group means
ggplot() +
  # add density lines for each distribution
  geom_line(aes(x = x, y = dnorm(x, mu1, sigma2)), 
            col = "blue", lwd = 1.2) +
  geom_line(aes(x = x, y = dnorm(x, mu2, sigma2)), 
            col = "orange", lwd = 1.2) +
  geom_line(aes(x = x, y = dnorm(x, mu3, sigma2)), 
            col = "maroon", lwd = 1.2) +
  # add 3 group mean lines
  geom_vline(xintercept = c(mu1, mu2, mu3), lty = "dashed") +
  labs(
    title = "Plots of three distributions",
    subtitle = "dashed lines = means for each of 3 groups",
    y = "density"
  ) +
  theme_classic()
```

## 3. $F$-Statistic

The $F$-ratio is calculated using these sums of squares:

$$
F=\frac{\text{SSB}/\text{df}_B}{\text{SSW}/\text{df}_W} = \frac{\text{Var}_B}{\text{Var}_W}
$$

If the group means are close together, the SSB will be small relative to the SSW. Looking at the plot below of a different hypothetical scenario, the group means are all right around 1. The variation among the three dotted lines is smaller than the variation within any of the three distributions. In this case, the $F$-ratio will be quite small. 

```{r plot-ssb-small}
# plot of three distributions with small SSB
mu1_small = 1; mu2_small = 1.25; mu3_small = 0.75
ggplot() +
  # add density lines for each distribution
  geom_line(aes(x = x, y = dnorm(x, mu1_small, sigma2)), 
            col = "blue", lwd = 1.2) +
  geom_line(aes(x = x, y = dnorm(x, mu2_small, sigma2)), 
            col = "orange", lwd = 1.2) +
  geom_line(aes(x = x, y = dnorm(x, mu3_small, sigma2)), 
            col = "maroon", lwd = 1.2) +
  # add 3 group mean lines
  geom_vline(xintercept = c(mu1_small, mu2_small, mu3_small), lty = "dashed") +
  labs(
    title = "Plots of three distributions",
    subtitle = "dashed lines = means for each of 3 groups",
    y = "density"
  ) +
  theme_classic()
```

But as the SSB increases, the $F$-ratio gets successively larger because the variation of the group means around the grand mean gets successively larger than the variation of within-group observations around the corresponding group mean. Because it is a ratio ranging from $0 \rightarrow \infty$, a larger $F$-statistic is considered more extreme. As a result, as the $F$-ratio increases, we are more likely to reject the null hypothesis and find evidence of mean differences.

## 4. Conduct the test with `aov()`

Let's go through an example of conducting a one-way ANOVA in R. The `PlantGrowth` dataframe from the `{datasets}` package comprises of $N=30$ plants. There is one treatment variable with three levels: (a) control, (b) treatment 1, and (c) treatment 2. We will test whether there are differences among the plant yields as a function of the treatment condition.

```{r data-anova}
# Load data
data("PlantGrowth")

# Data summary
summary(PlantGrowth)

# Box plot
ggplot(data = PlantGrowth,
       aes(x = group,
           y = weight)) +
  geom_boxplot(fill = "maroon", alpha = 0.2) +
  labs(
    title = "Box Plot of Plant Weight by Treatment Type",
    y = "Plant Weight",
    x = "Treatment Type"
  ) +
  theme_classic()
```

The above plot indicates differing means among the three conditions. Specifically, plant weight seems smaller when treatment 1 is applied compared to treatment 2 or the control condition. But are these means significantly different (more so than by chance)?

There are two main ways to run an ANOVA in R. The first is to use the `aov()` function. We need to provide a formula to this formula in the general syntax of `aov(outcome_variable ~ grouping_variable, data = data_name`. This means that the data should be in long format, with multiple rows for each subject. The outcome variable is also called the *dependent variable* and the grouping variable is also called the *independent variable*. The anova object is often stored in a variable and summary information can be accessed with the `summary()` function.

```{r aov}
# Fit model and store it in an object
plantmod <- aov(weight ~ group, data = PlantGrowth)

# ANOVA table
summary(plantmod)
```

In the above ANOVA table, the SSB estimate is 3.766 and the SSW estimate is 10.492. The mean square (MS) estimates are computed by dividing the SS estimates by the corresponding degrees of freedom (e.g., MS for `group` is $3.7663/2 = 1.8832$). Our $F$-ratio is then calculated using the mean square estimates. The degrees of freedom for SSB is the number of groups ($k$) minus 1, $3-1=2$. Our degrees of freedom for SSW is the total number of observations ($n$) minus $k$, $30-3=27$. Note that we can calculate the $p$-value for the $F$-ratio using the `pf()` function, with similar motivation as the `pnorm()` function for $z$-test and `pt()` function for $t$-test! Importantly, $F$-statistic is always positive because it is a ratio, so `lower.tail` argument should always be set to `FALSE` or accessed via `1 - pf()`.

```{r p-val-aov}
# p-value
1 - pf(4.846, 2, 27)

# Alternative way
pf(4.846, 2, 27, lower.tail = FALSE)
```

With $\alpha=0.05$, we reject the null hypothesis that the group means are equivalent. similarly, you can find a critical $F$-value with `qf()` and compare that against our $F$-statistic of 4.846. Again, these 2 decisions should *always* agree.

```{r f-crit-aov}
# f critical value
qf(p = 0.05, df1 = 2, df2 = 27, lower.tail = FALSE)
```

## 5. Conduct the test with `lm()`

Here's a fun fact: an ANOVA is actually a type of linear regression! So the second way to run an ANOVA in R is to use the `lm()` function, which stands for **linear models**. The syntax is strikingly similar to `aov()`.

```{r lm}
# Run ANOVA with lm and store in an object
plantmodlm <- lm(weight ~ group, data = PlantGrowth)

# Model summary
summary(plantmodlm)

# ANOVA table
anova(plantmodlm)
```

Rejecting the null hypothesis for the omnibus $F$-test only tells us that at least one of the mean values differs. We must do post-hoc analyses to better understand the pattern of our means. That is, which of the 3 treatment groups actually differ from each other? We will talk about that soon!

# III. Miscellaneous R Tips: LaTex Symbols

We've seen how LaTeX code can be used in R Markdown files to create symbols and mathematical equations. Like R, learning LaTeX has a learning curve. If you're interested in bettering your knowledge of LaTeX, here are some symbols that you might frequently use. Remember that to add LaTeX "in line" in a Markdown file, put a single dollar sign before and after the LaTeX code. 

- `$\mu$` $\rightarrow$ $\mu$
- `$\mu_1$` $\rightarrow$ $\mu_1$
- `$\mu_{abc}$` $\rightarrow$ $\mu_{abc}$
- `$\sigma$` $\rightarrow$ $\sigma$
- `$\sigma^2$` $\rightarrow$ $\sigma^2$
- `$\alpha$` $\rightarrow$ $\alpha$
- `$\mathbf{x}$` $\rightarrow$ $\mathbf{x}$
- `$\neq$` $\rightarrow$ $\neq$
- `$\leq$` $\rightarrow$ $\leq$
- `$\sim$` $\rightarrow$ $\sim$
- `$\in$` $\rightarrow$ $\in$
- `$\infty$` $\rightarrow$ $\infty$
- `$\subset$` $\rightarrow$ $\subset$
- `$\frac{1}{2}$` $\rightarrow$ $\frac{1}{2}$
- `$\rightarrow$`  $\rightarrow$  $\rightarrow$

To have LaTeX code on its own line (and centered), surround the code with two dollar signs on each side.

`$$y = ax + b$$` $\rightarrow$ $$y = ax + b$$

To number equations, use a special LaTeX environment for equations. You can also align multi-line equations so they look neat:

```
\begin{equation}
  \begin{aligned}
y &= ax + b\\
  &= 7x + 2
  \end{aligned}
\end{equation}
```

\begin{equation}
  \begin{aligned}
y &= ax + b\\
  &= 7x + 2
  \end{aligned}
\end{equation}

# V. References

- This document is adapted from materials from Justin Kracht and Allie Cooperman
