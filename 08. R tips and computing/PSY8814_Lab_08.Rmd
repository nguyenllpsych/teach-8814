---
title: "PSY 8814 - Lab 08: R Tips and Computing"
author: "Linh Nguyen"
date: "2023-10-27"
output: 
  pdf_document:
    toc: TRUE
urlcolor: blue
---

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      fig.height = 4)
```

```{r libraries, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(parallel)
library(tidyr)
```


# I. Computing Resources at UMN

Sometimes, you might need to perform tasks or analyses in R that are very computing-intensive. The University has multiple resources for high-powered compuing that you could use. Here, I introduce computing clusters that are available on campus, and link to webpages with a plethora of information to get you started!

## LATIS

[Liberal Arts Technologies and Innovation Services](https://cla.umn.edu/latis) (LATIS) supports learning and research within the College of Liberal Arts. LATIS offers a computing server in the Linux environment called `compute.cla`. You can use an interactive session, where you load applications through modules and directly interface with those applications. Alternatively, you can submit a job with a PBS script to run your analyses. 

To use `compute.cla`, CLA students should first register an account [here](https://latisresearch.umn.edu/linux-account). You can connect to `lts.cla.umn.edu` (the gateway to `compute.cla`) using NoMachine (recommended) or via SSH. Then, you can start your session with compute.cla. You can specify the resources that you'd like for each job, including the runtime (as long as two weeks in some cases), memory for storage, and number of nodes/cores on which your job should run. This is especially useful for large simulation studies (common in quant) or studies with large samples (common in behavior genetics).

Once you have an account, you'll have a home directory on which you can store your R scripts and saved data. Please check with your advisor and LATIS if you are considering storing sensitive data in your home directory. 

Here are great resources for learning more about, and using, `compute.cla`:

- [Introductory guide](https://latisresearch.umn.edu/cla-compute-cluster-intro)
- [Connecting via NoMachine](https://latisresearch.umn.edu/connecting-nomachine)
- [Transfering files via SFTP](https://latisresearch.umn.edu/connecting-sftp)
- [Creating a PBS script](https://latisresearch.umn.edu/creating-a-PBS-script)
- [Submitting jobs](https://latisresearch.umn.edu/submitting-jobs-computecla)

## MSI

Another resource on campus for high-powered computing is the [Minnesota Supercomputing Institute](https://www.msi.umn.edu/). Your principal investigator can help you get access to the MSI (see [this page](https://www.msi.umn.edu/content/eligibility-getting-access) for details). The MSI has both interactive and non-interactive computing services, many within a Linux environment. As with `compute.cla`, the MSI is a great resource for analyses that are computationally-expensive and/or require specialized software. 

- [Getting started](https://www.msi.umn.edu/getting-started)
- [High Performance Computing services](https://www.msi.umn.edu/content/hpc)
- [Interactive High Performance Computing services](https://www.msi.umn.edu/content/interactive-hpc)

# II. Parallel Processing

We often want to consider computing efficiency when designing analyses. If we have a computing-intensive analysis that requires many resources and a lot of time, we can think of ways in which to "speed up" the analysis. R and RStudio have ways to *profile* code to identify the functions that are most intensive.

Another process for increasing the efficiency of our analysis is parallel processing. As an example, let's say we have a for-loop with 1,000 repetitions. We've seen this for-loop programmed sequentially, where R runs repetition 1, then repetition 2, and so on. Our computers are typically using one core to run this process. 

If our repetitions are independent, we can leverage the multiple other cores within our computers (or an even more robust computing cluster system like `compute.cla`). With multiple cores working at once, multiple repetitions of the for-loop can be run at the same time. Specifically, on a laptop with four cores, we might have four conditions running at once (one on each core). Parallel processing can be extremely helpful for those time- and computing-intensive analyses! Importantly, parallel processing is not only applicable for making for-loops run faster. You can "parallelize" many analyses!

There are many ways to use parallel processing in R. A common function is `mclapply()` or `parLapply()` in the `{parallel}` package. This function is a parallel version of `lapply()`, and outputs a list of results. The `{foreach}` package (Wallig et al., 2020) also has functionality for parallel processing. 

```{r parallel-setup}
# create a function for repeated sampling of the uniform distribution
#   to create the sampling distribution of the mean
my_function <- function(icondition,
                        seed = 8814) {
  
  # pull important info from condition_mat at the current icondition
  min_val     <- condition_mat[icondition, "min_val"]
  max_val     <- condition_mat[icondition, "max_val"]
  sample_size <- condition_mat[icondition, "sample_size"]
  reps        <- condition_mat[icondition, "reps"]
  
  # create a vector for the mean
  sample_means <- numeric(length = reps)
  
  # run the repeated sampling process for the current condition
  for(i in 1:reps){
    
    # sample data
    current_sample <- runif(n = sample_size, min = min_val, max = max_val)
    
    # store mean
    sample_means[i] <- mean(current_sample)
    
  } # END for ireps LOOP
  
  # create a histogram of the means
  my_plot <- ggplot(data = data.frame(sample_means),
                    aes(sample_means)) +
  geom_histogram(fill = "#7A0019") +
  labs(
    title = "Sampling distribution of the mean of the uniform distribution",
    subtitle = paste("with sample size n =", sample_size, 
                     "across", reps, "repetitions")
  ) +
  theme_classic()
  
  # save the plot to a local folder
  # must have been created before
  ggsave(filename = paste0("plots/plot_n", sample_size, "_rep", reps, ".jpg"))
  
  
} # END my_function DEF

# create a matrix of conditions to test the central limit theorem
condition_mat <- expand.grid(min_val = 0,
                             max_val = 1,
                             sample_size = c(5, 10, 50, 100, 250, 500, 1000, 5000),
                             reps = c(seq(from = 50, to = 1000, by = 100)))

# take a look at the first few conditions
head(condition_mat)

# how many conditions are there in total
(n_condition <- nrow(condition_mat))
```

Let's run through the conditions in parallel! So the conditions are split among the number of cores on the computer.

```{r parallel, message = FALSE, warning=FALSE, results=FALSE}
# create clusters
n_core <- detectCores()
cl     <- makeCluster(n_core)

# load libraries in clusters
clusterEvalQ(cl, library(ggplot2))

# send condition matrix to clusters
clusterExport(cl, "condition_mat")

# run simulation tests
start_par <- Sys.time()
parLapply(cl, 1:n_condition, my_function)
end_par <- Sys.time()

# stop cluster
stopCluster(cl)
```

Let's now run through the conditions *not* in parallel! So just going through them one by one in a for loop.

```{r parallel-none, message = FALSE, warning=FALSE, results=FALSE}
# loop through number of conditions
start_normal <- Sys.time()
for(icondition in 1:n_condition){
  my_function(icondition = icondition)
}
end_normal <- Sys.time()
```

We can now compare how long the process takes with and without parallel processing.

```{r parallel-compare}
# runtime for parallel
end_par - start_par

# runtime for non-parallel
end_normal - start_normal
```

# III. Reshaping Data

Our data are often in one of two forms:

- **Long data**: Multiple rows per unit of analysis
- **Wide data**: One row per unit of analysis, with multiple columns for different variables 

As an example, consider longitudinal data where participants are evaluated at three time points. When the data are in long format, we have three rows per participant. Each row has data for that participant from a different time point. When the data are in wide format, we have one row per participant. Then different column variables denote data at the different time points.

In many analyses, we may need to *reshape* our data from wide to long or from long to wide. There are multiple ways to reshape data available in R. Here, I'll go over the `pivot_wider()` and `pivot_longer()` functions from the `{tidyr}` package (Wickham, 2021).

Let's start by creating a simulated data set with 10 participants across three time points. I'll create the data in long format.

```{r}
# Create simulated data set
simdata <- data.frame(ID = rep(1:10, each = 3),
                      Timepoint = rep(1:3, times = 10),
                      item1 = sample(1:6, size = 30, replace = T),
                      item2 = sample(1:6, size = 30, replace = T),
                      item3 = sample(1:6, size = 30, replace = T))

# Look at the first six observations
head(simdata)
```

## Long to wide

The `tidyr::pivot_wider()` function reshapes data from long to wide format.

```{r long-to-wide}
# Reshape data long to wide with tidyr
simdata.wide2 <- simdata %>%
  pivot_wider(names_from = "Timepoint",
              values_from = c("item1", "item2", "item3"),
              names_sep = "_")

# Look at first three observations
# Items are ordered by item number, not time point
head(simdata.wide2, n = 3)
```

## Wide to long

The `tidyr::pivot_longer()` function reshapes data from wide to long format.

```{r wide-to-long}
# Reshape data wide to long with tidyr
simdata.long2 <- simdata.wide2 %>%
  pivot_longer(cols = starts_with("item"),
               names_to = c(".value", "time"),
               names_sep = "_")

# Look at first three observations
head(simdata.long2, n = 3)

# Making the data frame even longer
simdata.long3 <- simdata.wide2 %>%
  pivot_longer(cols = starts_with("item"),
               names_to = c(".value", "time"),
               names_sep = "_") %>%
  pivot_longer(cols = starts_with("item"),
               names_to = "item",
               values_to = "score")

# Look at first three observations
head(simdata.long3)
```

[This vignette](https://tidyr.tidyverse.org/articles/pivot.html) provides a lot of helpful information on reshaping your data with functions from the `{tidyr}` package!

# IV. RMarkdown Knitting

## PDF formatting

Knitting the base RMarkdown document already provides you with quite a pretty document. However, there are ways to enhance your formatting and organization. You can do this editing the YAML header at the beginning of the .Rmd file.

You can add a table of content with specific depth, specifying url colors, figure widths and height, and even use custom templates.

````{verbatim}
---
title: "My document"
author: "My name"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: TRUE
urlcolor: blue
---
````

## HTML formatting and publishing

RMarkdown can serve an incredible number of purposes. It can write analytic report, websites, paper, books, and so on. Personally, I prefer knitting RMarkdown documents to a HTML document, which can then be published and shared via websites like [RPubs](https://rpubs.com/).

Similar to PDF formatting, you can edit the YAML header at the beginning of the .Rmd file, with additional functionality to add a floating table of content, code folding for readability, and so on. You can also use a custom css file for all your styling needs, if you're web-dev-inclined.

## Quarto: Next-generation RMarkdown

Posit (formerly RStudio) recently launched and is heavily promoting Quarto documents. They are supposed to be an over-powered RMarkdown, with an emphasis on combining multiple programming languages. You can create your own using the base template in RStudio.

I am extremely new to Quarto myself, but I've seen great things done using this format, including full manuscript referencing many different code files in different languages, and the interactive webpage that I tested a few weeks back. For a good summary of the benefits of Quarto over RMarkdown, you can check out this [StackOverflow discussion](https://stackoverflow.com/questions/72089640/what-are-the-benefits-of-using-quarto-over-rmarkdown).

Much to learn...

# V. References

- This document is adapted from materials from Justin Kracht and Allie Cooperman
- Wallig, M., Microsoft, & Weston, S. (2020). foreach: Provides foreach looping construct. R package version 1.5.1
- Wickham, H. (2021). tidyr: Tidy Messy Data. R package version 1.1.3.
- [R Markdown: The Definitive Guide - Yihui Xie](https://bookdown.org/yihui/rmarkdown/)